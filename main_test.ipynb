{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "import ipaddress  # For advanced IP address manipulation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_ip</th>\n",
       "      <th>dst_ip</th>\n",
       "      <th>bytes_out</th>\n",
       "      <th>bytes_in</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.179.92.14</td>\n",
       "      <td>206.71.188.20</td>\n",
       "      <td>1603462</td>\n",
       "      <td>121</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.214.74.202</td>\n",
       "      <td>187.116.99.103</td>\n",
       "      <td>778167</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.52.1.87</td>\n",
       "      <td>137.129.191.187</td>\n",
       "      <td>1784372</td>\n",
       "      <td>160</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.57.21.252</td>\n",
       "      <td>188.48.218.58</td>\n",
       "      <td>1933257</td>\n",
       "      <td>475</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.14.189.189</td>\n",
       "      <td>150.107.54.243</td>\n",
       "      <td>1055839</td>\n",
       "      <td>504</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          src_ip           dst_ip  bytes_out  bytes_in  hour_of_day  label\n",
       "0   10.179.92.14    206.71.188.20    1603462       121            6      1\n",
       "1  10.214.74.202   187.116.99.103     778167       130           18      1\n",
       "2     10.52.1.87  137.129.191.187    1784372       160           21      1\n",
       "3   10.57.21.252    188.48.218.58    1933257       475           11      1\n",
       "4  10.14.189.189   150.107.54.243    1055839       504           15      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_synthetic_network_data(num_samples=10000, anomaly_ratio=0.01):\n",
    "    \"\"\"\n",
    "    Generates a synthetic dataset of network connections. \n",
    "    A small fraction (anomaly_ratio) simulate possible data exfil (anomalies).\n",
    "    \"\"\"\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data = {\n",
    "        'src_ip': [],\n",
    "        'dst_ip': [],\n",
    "        'bytes_out': [],\n",
    "        'bytes_in': [],\n",
    "        'hour_of_day': [],\n",
    "        'label': []  # 0 = normal, 1 = potential exfil/anomaly\n",
    "    }\n",
    "    \n",
    "    num_anomalies = int(num_samples * anomaly_ratio)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        hour = np.random.randint(0, 24)\n",
    "        is_anomaly = (i < num_anomalies)\n",
    "        data['label'].append(int(is_anomaly))\n",
    "        \n",
    "        if is_anomaly:\n",
    "            # Potential exfil: large bytes_out, traffic to an external global IP\n",
    "            src_ip = f\"10.{np.random.randint(0,255)}.{np.random.randint(0,255)}.{np.random.randint(0,255)}\"\n",
    "            dst_ip = f\"{np.random.randint(100,255)}.{np.random.randint(0,255)}.{np.random.randint(0,255)}.{np.random.randint(0,255)}\"\n",
    "            bytes_out = np.random.randint(500000, 2000000)  # large data transfer\n",
    "            bytes_in  = np.random.randint(0, 1000)\n",
    "        else:\n",
    "            # Normal traffic stays in private range\n",
    "            src_ip = f\"10.{np.random.randint(0,255)}.{np.random.randint(0,255)}.{np.random.randint(0,255)}\"\n",
    "            dst_ip = f\"10.{np.random.randint(0,255)}.{np.random.randint(0,255)}.{np.random.randint(0,255)}\"\n",
    "            bytes_out = np.random.randint(0, 50000)\n",
    "            bytes_in  = np.random.randint(0, 50000)\n",
    "\n",
    "        data['src_ip'].append(src_ip)\n",
    "        data['dst_ip'].append(dst_ip)\n",
    "        data['bytes_out'].append(bytes_out)\n",
    "        data['bytes_in'].append(bytes_in)\n",
    "        data['hour_of_day'].append(hour)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = generate_synthetic_network_data(num_samples=10000, anomaly_ratio=0.01)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IP Feature Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ip_features(ip_str):\n",
    "    \"\"\"\n",
    "    Convert an IP string into a vector of features using `ipaddress` library.\n",
    "    \n",
    "    Rationale behind each feature:\n",
    "      - Checking whether an IP is private, global, loopback, etc., can help \n",
    "        detect unusual or out-of-policy traffic in anomaly detection use cases.\n",
    "      - Identifying the IP version is vital for IPv6 vs. IPv4 scenarios, which \n",
    "        can differ in routing and security contexts.\n",
    "      - The integer representation often feeds machine learning models better \n",
    "        than a raw string, allowing potential numeric patterns (e.g., subnets).\n",
    "        \n",
    "    Returns a list of numeric features in this order:\n",
    "      [\n",
    "        is_private, \n",
    "        is_global,\n",
    "        is_loopback,\n",
    "        is_link_local,\n",
    "        is_reserved,\n",
    "        is_multicast,\n",
    "        ip_version,\n",
    "        integer_representation\n",
    "      ]\n",
    "    \"\"\"\n",
    "    # Convert the string to an ipaddress object (handles both IPv4Address and IPv6Address).\n",
    "    # Why: We need a structured representation to query properties like is_private, is_loopback, etc.\n",
    "    ip_obj = ipaddress.ip_address(ip_str)\n",
    "    \n",
    "    # is_private: 1 if IP is in an RFC-defined private range (e.g. 10.x, 192.168.x).\n",
    "    # Why: Traffic to/from private IPs often indicates internal-only communication. \n",
    "    #      A sudden spike in external (non-private) IP usage could signal data exfiltration or suspicious activity.\n",
    "    is_private = int(ip_obj.is_private)\n",
    "    \n",
    "    # is_global: 1 if IP is a globally routable address.\n",
    "    # Why: Important for anomaly detection—traffic to global IPs might be suspicious \n",
    "    #      if the environment typically restricts external connections or uses VPN tunnels.\n",
    "    is_global = int(ip_obj.is_global)\n",
    "    \n",
    "    # is_loopback: 1 if IP is in the loopback range (127.x.x.x for IPv4).\n",
    "    # Why: Loopback addresses typically mean local traffic. If loopback traffic appears \n",
    "    #      unexpectedly in logs, it might be a misconfiguration or malicious tunneling.\n",
    "    is_loopback = int(ip_obj.is_loopback)\n",
    "    \n",
    "    # is_link_local: 1 if IP is in the link-local range (169.254.x.x).\n",
    "    # Why: Link-local addresses are auto-configured, often used in internal subnets \n",
    "    #      without a DHCP server. Unusual link-local usage might signal misconfigurations.\n",
    "    is_link_local = int(ip_obj.is_link_local)\n",
    "    \n",
    "    # is_reserved: 1 if IP is in a reserved block (non-routable or set aside for special use).\n",
    "    # Why: Reserved addresses usually shouldn't appear in normal production traffic. \n",
    "    #      Their presence could indicate an error or nefarious testing by an attacker.\n",
    "    is_reserved = int(ip_obj.is_reserved)\n",
    "    \n",
    "    # is_multicast: 1 if IP is in the multicast address range (224.0.0.0 to 239.255.255.255 for IPv4).\n",
    "    # Why: Multicast is uncommon in many environments. Unexpected multicast traffic \n",
    "    #      can be a clue for suspicious network patterns or advanced tunneling.\n",
    "    is_multicast = int(ip_obj.is_multicast)\n",
    "    \n",
    "    # ip_version: 4 or 6. IPv6 has different security and routing considerations than IPv4.\n",
    "    # Why: Knowing the version helps handle logs differently and might impact how \n",
    "    #      anomaly thresholds are set—IPv6 can use huge address spaces or unique subnets.\n",
    "    ip_version = ip_obj.version\n",
    "    \n",
    "    # Convert IP to an integer. For IPv4, it's a 32-bit number; for IPv6, 128-bit.\n",
    "    # Why: ML models often prefer numeric input. It can also reveal patterns \n",
    "    #      (e.g., close numeric ranges may map to the same subnet).\n",
    "    ip_int = int(ip_obj)\n",
    "    \n",
    "    return [\n",
    "        is_private,\n",
    "        is_global,\n",
    "        is_loopback,\n",
    "        is_link_local,\n",
    "        is_reserved,\n",
    "        is_multicast,\n",
    "        ip_version,\n",
    "        ip_int\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new features to full dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of scaled input: (10000, 19)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_dataframe(df):\n",
    "    \"\"\"\n",
    "    1. Extract advanced IP features for both src_ip and dst_ip using `ipaddress`.\n",
    "    2. Concatenate them with numeric columns (bytes_out, bytes_in, hour_of_day).\n",
    "    3. Scale features using MinMaxScaler for improved training stability.\n",
    "    4. Return a NumPy array of scaled features and the fitted scaler.\n",
    "    \"\"\"\n",
    "    # Extract IP-based features for src_ip\n",
    "    src_ip_features = df['src_ip'].apply(extract_ip_features).tolist()\n",
    "    # Each element is an 8-dimensional feature vector from above\n",
    "    \n",
    "    # Extract IP-based features for dst_ip\n",
    "    dst_ip_features = df['dst_ip'].apply(extract_ip_features).tolist()\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    src_ip_array = np.array(src_ip_features, dtype=float)\n",
    "    dst_ip_array = np.array(dst_ip_features, dtype=float)\n",
    "    \n",
    "    # Combine with numeric columns\n",
    "    numeric_cols = ['bytes_out', 'bytes_in', 'hour_of_day']\n",
    "    numeric_data = df[numeric_cols].values.astype(float)\n",
    "    \n",
    "    # Final feature matrix\n",
    "    X = np.hstack([src_ip_array, dst_ip_array, numeric_data])\n",
    "    \n",
    "    # Scale\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Preprocess the dataframe\n",
    "X_scaled, scaler = preprocess_dataframe(df)\n",
    "print(\"Shape of scaled input:\", X_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train the Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkDataset(Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = torch.from_numpy(features).float() # Converts the NumPy array (features) to a float tensor so PyTorch can handle it\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0] # Returns the dataset size (number of samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx] # Retrieves one sample (feature vector) from the dataset by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A lightweight PyTorch Dataset that wraps a NumPy array of features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features):\n",
    "        \"\"\"\n",
    "        Converts the NumPy array (features) to a float tensor so PyTorch can handle it.\n",
    "        \"\"\"\n",
    "        self.features = torch.from_numpy(features).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the dataset size (number of samples).\n",
    "        \"\"\"\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves one sample (feature vector) from the dataset by index.\n",
    "        \"\"\"\n",
    "        return self.features[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/val/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (8000, 19)\n",
      "Val shape: (1000, 19)\n",
      "Test shape: (1000, 19)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "X_val, X_test = train_test_split(X_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "train_dataset = NetworkDataset(X_train)\n",
    "val_dataset   = NetworkDataset(X_val)\n",
    "test_dataset  = NetworkDataset(X_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Val shape:\", X_val.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=16):\n",
    "        \"\"\"\n",
    "        Simple fully-connected Autoencoder:\n",
    "          - Encoder -> compress input_dim -> latent_dim\n",
    "          - Decoder -> reconstruct input_dim from latent_dim\n",
    "        \"\"\"\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.047325, Val Loss: 0.014266\n",
      "Epoch 2/10, Train Loss: 0.010884, Val Loss: 0.008656\n",
      "Epoch 3/10, Train Loss: 0.003454, Val Loss: 0.001188\n",
      "Epoch 4/10, Train Loss: 0.000329, Val Loss: 0.000517\n",
      "Epoch 5/10, Train Loss: 0.000216, Val Loss: 0.000462\n",
      "Epoch 6/10, Train Loss: 0.000187, Val Loss: 0.000398\n",
      "Epoch 7/10, Train Loss: 0.000157, Val Loss: 0.000332\n",
      "Epoch 8/10, Train Loss: 0.000126, Val Loss: 0.000289\n",
      "Epoch 9/10, Train Loss: 0.000099, Val Loss: 0.000216\n",
      "Epoch 10/10, Train Loss: 0.000081, Val Loss: 0.000219\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder(input_dim=X_train.shape[1], latent_dim=16)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_autoencoder(model, train_loader, val_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for batch_features in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_features)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_features in val_loader:\n",
    "                outputs = model(batch_features)\n",
    "                loss = criterion(outputs, batch_features)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, \"\n",
    "              f\"Train Loss: {total_loss/len(train_loader):.6f}, \"\n",
    "              f\"Val Loss: {val_loss/len(val_loader):.6f}\")\n",
    "\n",
    "train_autoencoder(model, train_loader, val_loader, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Scoring and Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Reconstruction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reconstruction_errors(model, dataloader):\n",
    "    model.eval()\n",
    "    errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_features in dataloader:\n",
    "            reconstruction = model(batch_features)\n",
    "            # MSE per sample\n",
    "            batch_loss = (reconstruction - batch_features).pow(2).mean(dim=1)\n",
    "            errors.append(batch_loss.cpu().numpy())\n",
    "    \n",
    "    errors = np.concatenate(errors)\n",
    "    return errors\n",
    "\n",
    "# 1. Compute reconstruction errors on the test set\n",
    "test_errors = compute_reconstruction_errors(model, test_loader)\n",
    "\n",
    "# 2. Choose a threshold (e.g., 95th percentile)\n",
    "threshold = np.percentile(test_errors, 95)\n",
    "\n",
    "# 3. Flag anomalies (1) if the error exceeds threshold\n",
    "anomaly_flags = (test_errors > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataframe for viewing anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp_df = df.iloc[len(X_train):].reset_index(drop=True)  # combined val+test portion\n",
    "split_idx = len(X_val)  # the portion for validation\n",
    "df_test = X_temp_df.iloc[split_idx:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['anomaly_score'] = test_errors\n",
    "df_test['anomalous'] = anomaly_flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort by anomaly score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_ip</th>\n",
       "      <th>dst_ip</th>\n",
       "      <th>bytes_out</th>\n",
       "      <th>bytes_in</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>label</th>\n",
       "      <th>anomaly_score</th>\n",
       "      <th>anomalous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>10.66.110.75</td>\n",
       "      <td>10.249.240.225</td>\n",
       "      <td>27440</td>\n",
       "      <td>44504</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009384</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>10.68.42.97</td>\n",
       "      <td>10.49.212.3</td>\n",
       "      <td>18795</td>\n",
       "      <td>42275</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009367</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>10.144.16.93</td>\n",
       "      <td>10.81.52.230</td>\n",
       "      <td>32575</td>\n",
       "      <td>18028</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007328</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>10.133.85.41</td>\n",
       "      <td>10.42.141.203</td>\n",
       "      <td>280</td>\n",
       "      <td>18899</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>10.35.50.188</td>\n",
       "      <td>10.146.80.103</td>\n",
       "      <td>32124</td>\n",
       "      <td>32650</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005658</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>10.146.25.206</td>\n",
       "      <td>10.172.251.9</td>\n",
       "      <td>9825</td>\n",
       "      <td>20478</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>10.223.253.27</td>\n",
       "      <td>10.217.99.60</td>\n",
       "      <td>28199</td>\n",
       "      <td>41141</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>10.179.131.130</td>\n",
       "      <td>10.140.20.7</td>\n",
       "      <td>887</td>\n",
       "      <td>19656</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>10.96.135.189</td>\n",
       "      <td>10.18.35.111</td>\n",
       "      <td>740</td>\n",
       "      <td>28868</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>10.93.210.139</td>\n",
       "      <td>10.216.171.204</td>\n",
       "      <td>22339</td>\n",
       "      <td>15290</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             src_ip          dst_ip  bytes_out  bytes_in  hour_of_day  label  \\\n",
       "88     10.66.110.75  10.249.240.225      27440     44504            7      0   \n",
       "296     10.68.42.97     10.49.212.3      18795     42275            7      0   \n",
       "561    10.144.16.93    10.81.52.230      32575     18028            1      0   \n",
       "567    10.133.85.41   10.42.141.203        280     18899            5      0   \n",
       "904    10.35.50.188   10.146.80.103      32124     32650           23      0   \n",
       "574   10.146.25.206    10.172.251.9       9825     20478           14      0   \n",
       "255   10.223.253.27    10.217.99.60      28199     41141            1      0   \n",
       "42   10.179.131.130     10.140.20.7        887     19656            9      0   \n",
       "509   10.96.135.189    10.18.35.111        740     28868           18      0   \n",
       "857   10.93.210.139  10.216.171.204      22339     15290           17      0   \n",
       "\n",
       "     anomaly_score  anomalous  \n",
       "88        0.009384          1  \n",
       "296       0.009367          1  \n",
       "561       0.007328          1  \n",
       "567       0.005766          1  \n",
       "904       0.005658          1  \n",
       "574       0.005011          1  \n",
       "255       0.004394          1  \n",
       "42        0.003783          1  \n",
       "509       0.002104          1  \n",
       "857       0.001573          1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anomalies_ranked = df_test.sort_values(by='anomaly_score', ascending=False)\n",
    "\n",
    "# Display the top 10 most anomalous records\n",
    "df_anomalies_ranked.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
